{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33f5f4a",
   "metadata": {},
   "source": [
    "\n",
    "# 01 — Dimensionality Reduction\n",
    "\n",
    "**Milestone 2** · Power Outage Prediction\n",
    "\n",
    "This notebook reduces the dataset's dimensionality by selecting a compact, **leak-free** set of features suitable for modeling. It supports multiple selection strategies (univariate F-test, Mutual Information), correlation pruning, and optional PCA on continuous features. It saves a reduced dataset and an ordered list of final features for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ee89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dir: /Users/ayahalmusaddy/projects/AI-Studio-Project/data/processed\n",
      "Outputs dir  : /Users/ayahalmusaddy/projects/AI-Studio-Project/notebooks/outputs/milestone_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Setup ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Project paths (this notebook expected to live in notebooks/milestone_2/)\n",
    "PROCESSED = Path(\"../../data/processed\")\n",
    "OUTPUTS = Path(\"../../notebooks/outputs/milestone_2\")\n",
    "OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Processed dir:\", PROCESSED.resolve())\n",
    "print(\"Outputs dir  :\", OUTPUTS.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222df6c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053c6051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: integrated_dataset.csv with shape (3973578, 17)\n",
      "Columns: ['fips_code', 'date', 'prcp', 'tmax', 'tmin', 'outage_occurred', 'customers_out', 'county', 'state', 'run_start_time', 'year', 'month'] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try to load the modeling matrix produced in earlier notebooks.\n",
    "# Fallback to integrated dataset if the modeling matrix isn't available.\n",
    "model_df_path = PROCESSED / \"modeling_matrix.parquet\"\n",
    "integrated_csv = PROCESSED / \"integrated_dataset.csv\"\n",
    "\n",
    "if model_df_path.exists():\n",
    "    df = pd.read_parquet(model_df_path)\n",
    "    source = \"modeling_matrix.parquet\"\n",
    "elif integrated_csv.exists():\n",
    "    df = pd.read_csv(integrated_csv)\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    source = \"integrated_dataset.csv\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"No data found in ../../data/processed (need modeling_matrix.parquet or integrated_dataset.csv).\")\n",
    "\n",
    "print(f\"Loaded: {source} with shape {df.shape}\")\n",
    "print(\"Columns:\", list(df.columns)[:12], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03d416",
   "metadata": {},
   "source": [
    "## Define target and exclude leakage / IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4593a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature columns: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Target\n",
    "if 'outage_occurred' not in df.columns:\n",
    "    raise KeyError(\"Column 'outage_occurred' is required as the classification target.\")\n",
    "\n",
    "y = df['outage_occurred'].astype(int)\n",
    "\n",
    "# Columns we never use as features (IDs, text labels, or leakage)\n",
    "always_exclude = {\n",
    "    # IDs / metadata\n",
    "    'fips_code', 'county', 'state', 'region', 'date', 'county_name',\n",
    "    # Leakage (only known after event or derived from target)\n",
    "    'run_start_time', 'customers_out', 'outage_hour',\n",
    "    'state_risk_score', 'region_risk_score', 'state_risk_category', 'high_risk_state'\n",
    "}\n",
    "\n",
    "# Often excluded for time-based evaluation; keep cyclical encodings instead\n",
    "optional_exclude = {'year'}  # edit if you decide to include raw year\n",
    "\n",
    "exclude_cols = [c for c in df.columns if c in always_exclude or c in optional_exclude or c == 'outage_occurred']\n",
    "\n",
    "X = df.drop(columns=[c for c in exclude_cols if c in df.columns]).copy()\n",
    "print(\"Initial feature columns:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa1bd4",
   "metadata": {},
   "source": [
    "## Preprocess: encode categoricals and impute missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05ecefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-preprocess shape: (3973578, 7)\n",
      "Any NA left: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode remaining object/category columns\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Impute missing: numeric -> median; others -> mode\n",
    "for col in X.columns:\n",
    "    if X[col].isna().any():\n",
    "        if pd.api.types.is_numeric_dtype(X[col]):\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        else:\n",
    "            X[col] = X[col].fillna(X[col].mode().iloc[0])\n",
    "\n",
    "print(\"Post-preprocess shape:\", X.shape)\n",
    "print(\"Any NA left:\", int(X.isna().sum().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d4e15",
   "metadata": {},
   "source": [
    "## Univariate selection: F-test and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c803c34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved univariate scores -> ../../data/processed/feature_scores_univariate.csv\n",
      "Top-F (k): 7\n",
      "Top-MI(k): 7\n",
      "Union size: 7 | Intersection size: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "K = 30  # desired top-k from each univariate method (adjust later if needed)\n",
    "\n",
    "# F-test (assumes roughly linear/normal relationship)\n",
    "f_selector = SelectKBest(score_func=f_classif, k=min(K, X.shape[1]))\n",
    "f_selector.fit(X, y)\n",
    "f_scores = f_selector.scores_\n",
    "f_rank = pd.Series(f_scores, index=X.columns).sort_values(ascending=False).rename(\"f_score\")\n",
    "\n",
    "# Mutual Information (captures nonlinear dependencies)\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=min(K, X.shape[1]))\n",
    "mi_selector.fit(X, y)\n",
    "mi_scores = mi_selector.scores_\n",
    "mi_rank = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).rename(\"mi_score\")\n",
    "\n",
    "# Combine into a table\n",
    "rank_df = pd.concat([f_rank, mi_rank], axis=1).fillna(0)\n",
    "rank_df['rank_sum'] = rank_df['f_score'].rank(ascending=False) + rank_df['mi_score'].rank(ascending=False)\n",
    "rank_df = rank_df.sort_values(['rank_sum'], ascending=True)\n",
    "\n",
    "rank_df.to_csv(PROCESSED / \"feature_scores_univariate.csv\", index=True)\n",
    "print(\"Saved univariate scores ->\", PROCESSED / \"feature_scores_univariate.csv\")\n",
    "\n",
    "top_f  = f_rank.head(K).index.tolist()\n",
    "top_mi = mi_rank.head(K).index.tolist()\n",
    "print(\"Top-F (k):\", len(top_f))\n",
    "print(\"Top-MI(k):\", len(top_mi))\n",
    "\n",
    "# Candidate pools\n",
    "cand_union = list(dict.fromkeys(top_f + top_mi))  # preserve order, dedup\n",
    "cand_inter = [c for c in top_f if c in top_mi]\n",
    "print(\"Union size:\", len(cand_union), \"| Intersection size:\", len(cand_inter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb1e8b",
   "metadata": {},
   "source": [
    "## Correlation pruning (remove highly collinear features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2261733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning -> union: 6 intersection: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correlation_prune(cols, corr_threshold=0.90):\n",
    "    if len(cols) <= 1:\n",
    "        return cols\n",
    "    C = X[cols].corr().abs()\n",
    "    # Upper triangle mask\n",
    "    upper = C.where(np.triu(np.ones(C.shape), k=1).astype(bool))\n",
    "    to_drop = set()\n",
    "    for c in upper.columns:\n",
    "        if c in to_drop: \n",
    "            continue\n",
    "        high = upper[c][upper[c] > corr_threshold].index.tolist()\n",
    "        to_drop.update(high)\n",
    "    kept = [c for c in cols if c not in to_drop]\n",
    "    return kept\n",
    "\n",
    "union_pruned = correlation_prune(cand_union, corr_threshold=0.90)\n",
    "inter_pruned = correlation_prune(cand_inter, corr_threshold=0.90)\n",
    "print(\"After pruning -> union:\", len(union_pruned), \"intersection:\", len(inter_pruned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8eb43a",
   "metadata": {},
   "source": [
    "## Optional PCA on continuous features (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dad8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA numeric cols: 7 | components to reach 95% var: 6\n",
      "Saved PCA EVR -> ../../data/processed/pca_explained_variance.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll only run PCA on numeric columns as an auxiliary representation.\n",
    "# Save explained-variance info; do not force components into final feature set by default.\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(num_cols) >= 2:\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X[num_cols])\n",
    "    pca = PCA(n_components=None, random_state=42)\n",
    "    pca.fit(Xs)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    cum_evr = np.cumsum(evr)\n",
    "\n",
    "    # Choose #components to reach >=95% variance\n",
    "    n95 = int(np.searchsorted(cum_evr, 0.95) + 1)\n",
    "    pca_info = pd.DataFrame({\n",
    "        \"component\": np.arange(1, len(evr)+1),\n",
    "        \"explained_variance_ratio\": evr,\n",
    "        \"cumulative_evr\": cum_evr\n",
    "    })\n",
    "    pca_info.to_csv(PROCESSED / \"pca_explained_variance.csv\", index=False)\n",
    "    print(f\"PCA numeric cols: {len(num_cols)} | components to reach 95% var: {n95}\")\n",
    "    print(\"Saved PCA EVR ->\", PROCESSED / \"pca_explained_variance.csv\")\n",
    "else:\n",
    "    print(\"Not enough numeric columns for PCA (skipped).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3918e0",
   "metadata": {},
   "source": [
    "## Final feature selection (rule-based consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d951706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 6\n",
      "Final features: ['day_of_week', 'month', 'tmin', 'day_name', 'season', 'prcp']\n",
      "Saved -> ../../data/processed/final_features.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Consolidation rule:\n",
    "# 1) Start with intersection (robust across methods).\n",
    "# 2) If intersection < 12, add best from union until target size.\n",
    "TARGET_K = 18\n",
    "\n",
    "final_feats = inter_pruned.copy()\n",
    "for c in union_pruned:\n",
    "    if len(final_feats) >= TARGET_K:\n",
    "        break\n",
    "    if c not in final_feats:\n",
    "        final_feats.append(c)\n",
    "\n",
    "print(\"Final feature count:\", len(final_feats))\n",
    "print(\"Final features:\", final_feats)\n",
    "with open(PROCESSED / \"final_features.json\", \"w\") as f:\n",
    "    json.dump(final_feats, f, indent=2)\n",
    "print(\"Saved ->\", PROCESSED / \"final_features.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c368f39",
   "metadata": {},
   "source": [
    "## Save reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc1ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reduced dataset -> ../../data/processed/reduced_dataset.csv (3973578, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reduced = X[final_feats].copy()\n",
    "reduced['outage_occurred'] = y.values\n",
    "\n",
    "# Keep date and fips_code if available for later evaluation/analysis\n",
    "for extra in ['date','fips_code']:\n",
    "    if extra in df.columns and extra not in reduced.columns:\n",
    "        reduced[extra] = df[extra]\n",
    "\n",
    "reduced_path = PROCESSED / \"reduced_dataset.csv\"\n",
    "reduced.to_csv(reduced_path, index=False)\n",
    "print(\"Saved reduced dataset ->\", reduced_path, reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846ac95",
   "metadata": {},
   "source": [
    "## Notes / Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b614",
   "metadata": {},
   "source": [
    "\n",
    "- **Leakage removed:** `run_start_time`, `customers_out`, `outage_hour`, target-derived risk scores; IDs and labels removed.\n",
    "- **Univariate filters:** ANOVA F-test (linear) and Mutual Information (nonlinear). We kept `K=30` from each, then consolidated.\n",
    "- **Correlation pruning:** removed one of any feature pair with |corr| > 0.90.\n",
    "- **PCA (optional):** variance report saved; components not enforced in final set to preserve interpretability.\n",
    "- **Final feature count:** `TARGET_K = 18` by default; adjust if needed.\n",
    "- **Artifacts saved:**  \n",
    "  - `../../data/processed/feature_scores_univariate.csv`  \n",
    "  - `../../data/processed/pca_explained_variance.csv` (if applicable)  \n",
    "  - `../../data/processed/final_features.json`  \n",
    "  - `../../data/processed/reduced_dataset.parquet`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
